import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET
import random
import gzip
import xml.etree.ElementTree as ET
import os
import subprocess
import shutil
import json
import re



def get_valid_urls(url, break_url):
    """
    Get valid urls from sitemap.xml. Note: only links after group-49 are valid for download.
        - url: the link to stemap.xml
        - break_url: the url link to the group-49

    >>> url = "https://apkpure.com/sitemap.xml"
    >>> break_url = "https://apkpure.com/sitemaps/group-49.xml.gz"
    >>> valid_urls = get_valid_urls(url, break_url)
    >>> valid_urls[0]
    'https://apkpure.com/sitemaps/art_and_design.xml.gz'
    >>> len(valid_urls)
    7774
    """
    page = requests.get(url)
    page_text = page.text
    soup = BeautifulSoup(page.text, features="lxml")
    urls = [element.get_text() for element in soup.find_all('loc')]
    break_point = urls.index(break_url)
    valid_urls = urls[break_point+1:]
    return valid_urls

def sample_categories(valid_urls, num_categories):
    """
    Sample out a certain number of categories and put them into a dictionary with list storing their urls.
        - valid_urls: output of get_valid_urls() funciton
        - num_categories: number of categories that is specified in the configuration json file.
    """
    ## get all possible types of app.
    types = set()
    for url in valid_urls:
        try:
            type_ = re.search(r"sitemaps/(.*)-", url).group(1)
            types.add(type_)
        except:
            continue

    ## sample out the required number of categories from types and get them to a dictionary of list.
    categories = random.sample(types, num_categories)
    category_dict = {}
    for category in categories:
        for url in valid_urls:
            if category in url:
                category_dict.setdefault(category,[]).append(url)
    return category_dict

def sample_from_category(category_dict, num_to_sample):
    """
    Based on a category dictionary of urls, sample each category with the same number.
        - category_dict: a dictionary containing list of urls for each sampled category.
        - num_to_sample: the total number of urls needed to be sampled.
    """ 
    num_categories = len(category_dict.keys())
    num_to_sample_each = num_to_sample//num_categories
    for category in category_dict.keys():
        category_dict[category] = np.random.choice(category_dict[category], num_to_sample_each)
    return category_dict

def getAllLinksGZip(target_url):
    """
    From all GZip file links, sample one out and get all the url in it.
        - target_url: one of url in the sampled_urls

    >>> target_url = 'https://apkpure.com/sitemaps/game_arcade-49.xml.gz'
    >>> all_links = getAllLinksGZip(target_url)
    >>> len(all_links)
    1000
    """
    #target_url = random.sample(sampled_urls, 1)[0]
    resp = requests.get(target_url)
    data = gzip.decompress(resp.content)
    soup = BeautifulSoup(data, features='lxml')
    root = ET.fromstring(data)
    ## get all links in the sampled GZip xml file
    all_links = [i[0].text for i in root]
    return all_links


def getDownloadLink(all_links):
    """
    From all App links provided by decompressing the gZip file, sample one out and get the download link within.
        - all_links: all the links in one GZip xml file.

    >>> target_url = 'https://apkpure.com/sitemaps/game_arcade-49.xml.gz'
    >>> all_links = getAllLinksGZip(target_url)
    >>> download_link, title = getDownloadLink(all_links)
    >>> len(download_link) > 0
    True
    
    """
    app_page = random.sample(all_links, 1)[0]
    app_page_soup = BeautifulSoup(requests.get(app_page).text, features="lxml")
    a = app_page_soup.findAll("a", {"class": "da"})[0]
    try:
        ## if it has rel, if means that the app cannot be downloaded. Then recursively call this function to get a 
        ## valid link
        check = a['rel']
        return getDownloadLink(all_links)
    except:
        href = 'https://apkpure.com' + a['href']
        href_soup = BeautifulSoup(requests.get(href).text, features="lxml")
        title = app_page_soup.findAll("div", {"class": "title-like"})[0].find('h1').text
        href_a = href_soup.findAll("a", {"id": "download_link"})[0]
        return href_a['href'], title

def to_APK(download_link, title, apk_dir, category):
    """
    write to apk file.
        - download_link: the link to download the apk file of the app.
        - title: the name of the app.
        - apk_dir: the directory to write to.
        - category: the category directory that this file should be written to.
    """
    apk_file = requests.get(download_link)
    if not os.path.exists(apk_dir):
        os.mkdir(apk_dir)
    if not os.path.exists(apk_dir + '/' + category):
        os.mkdir(apk_dir + '/' + category)
    with open(apk_dir+'/' + category +'/'+ title + '.apk', 'wb') as f:
        f.write(apk_file.content)

def to_Smali(title, apk_dir, smali_dir, category):
    """
    get the smali code from apk file.
        - title: the name of the app.
        - apk_dir: the directory to get the apk file.
        - smali_dir: the directory to write to.
        - category: the category directory that this file should be written to.
    """
    if not os.path.exists(smali_dir):
        os.mkdir(smali_dir)
    if not os.path.exists(smali_dir + '/' + category):
        os.mkdir(smali_dir + '/' + category)
    subprocess.run(['apktool', 'd', apk_dir+'/'+ category +'/'+ title + '.apk', '-o', smali_dir + '/'+ category +'/'+ title], capture_output=True)

def del_Dir(title, smali_dir, category):
    """
    Only Smali directory and xml files are left. Delete irrelevant directories.
        - title: the name of the app, which is also the folder name containing the smali code.
        - smali_dir: the directory containing all the apps' smali code.
        - category: the category directory that this file should be written to.
    """
    all_directories = os.listdir( smali_dir + '/' + category +'/'+title)
    to_be_del = [directory for directory in all_directories if not (('.xml' in directory) or ("smali" in directory))]
    for directory in to_be_del:
        shutil.rmtree(smali_dir + '/'+ category + '/' + title+ '/' + directory, ignore_errors=True)

def get_data(url, break_url, num_to_sample, apk_dir, smali_dir, num_categories):
    valid_urls = get_valid_urls(url, break_url)
    category_dict = sample_categories(valid_urls, num_categories)
    sampled_category_dict = sample_from_category(category_dict, num_to_sample)
    
    for category in sampled_category_dict.keys():
        for target_url in sampled_category_dict[category]:
            all_links = getAllLinksGZip(target_url)
            download_link, title = getDownloadLink(all_links)
            to_APK(download_link, title, apk_dir, category)
            to_Smali(title, apk_dir, smali_dir, category)
            del_Dir(title, smali_dir, category)

cfg = json.load(open('data-injestion.json'))
get_data(**cfg)
