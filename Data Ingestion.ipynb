{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Design (10 Types for Each: some malware are mostly resides in certain types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### scrape based upon category with even distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get valid urls from sitemap.xml\n",
    "def get_valid_urls(url, break_url):\n",
    "    page = requests.get(url)\n",
    "    page_text = page.text\n",
    "    soup = BeautifulSoup(page.text)\n",
    "    urls = [element.get_text() for element in soup.find_all('loc')]\n",
    "    break_point = urls.index(break_url)\n",
    "    valid_urls = urls[break_point+1:]\n",
    "    return valid_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Out: Sample based on distribution of categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the category dict with each value as a list of urls belonging to that category\n",
    "category_list = {}\n",
    "for url in valid_urls:\n",
    "    try:\n",
    "        start = re.search(r'(sitemaps/)', test1).span()[1]\n",
    "        end = re.search(r'(-\\d)', url).span()[0]\n",
    "    except:\n",
    "        start = re.search(r'(sitemaps/)', test1).span()[1]\n",
    "        end = re.search(r'(.xml)', url).span()[0]\n",
    "    category = url[start:end]\n",
    "    if category in category_list.keys():\n",
    "        category_list[category].append(url)\n",
    "    else:\n",
    "        category_list[category] = [url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the category dict with each entry as the number of urls belonging to that category\n",
    "category_count = dict.fromkeys(category_list.keys())\n",
    "for key in category_count.keys():\n",
    "    category_count[key] = len(category_list[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Sample (Even Distribution over Category Types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sample out urls depending on the number of apps that we need\n",
    "import random\n",
    "def sample(num_to_sample, valid_urls):\n",
    "    ## sample without replacement\n",
    "    sampled_urls = random.sample(valid_urls, k = num_to_sample)\n",
    "    return sampled_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From all GZip file links, sample one out and get all the url in it.\n",
    "import gzip\n",
    "import xml.etree.ElementTree as ET\n",
    "def getAllLinksGZip(target_url):\n",
    "    #target_url = random.sample(sampled_urls, 1)[0]\n",
    "    resp = requests.get(target_url)\n",
    "    data = gzip.decompress(resp.content)\n",
    "    soup = BeautifulSoup(data, features='lxml')\n",
    "    root = ET.fromstring(data)\n",
    "    ## get all links in the sampled GZip xml file\n",
    "    all_links = [i[0].text for i in root]\n",
    "    return all_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From all App links provided by decompressing the gZip file, sample one out and get the download link within.\n",
    "def getDownloadLink(all_links):\n",
    "    app_page = random.sample(all_links, 1)[0]\n",
    "    app_page_soup = BeautifulSoup(requests.get(app_page).text)\n",
    "    a = app_page_soup.findAll(\"a\", {\"class\": \"da\"})[0]\n",
    "    try:\n",
    "        ## if it has rel, if means that the app cannot be downloaded. Then recursively call this function to get a \n",
    "        ## valid link\n",
    "        check = a['rel']\n",
    "        return getDownloadLink(all_links)\n",
    "    except:\n",
    "        href = 'https://apkpure.com' + a['href']\n",
    "        href_soup = BeautifulSoup(requests.get(href).text)\n",
    "        title = app_page_soup.findAll(\"div\", {\"class\": \"title-like\"})[0].find('h1').text\n",
    "        href_a = href_soup.findAll(\"a\", {\"id\": \"download_link\"})[0]\n",
    "        return href_a['href'], title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "## write to apk file\n",
    "import os\n",
    "def to_APK(download_link, title):\n",
    "    apk_file = requests.get(download_link)\n",
    "    if not os.path.exists('apk_files'):\n",
    "        os.mkdir('apk_files')\n",
    "    with open('apk_files/' + title + '.apk', 'wb') as f:\n",
    "        f.write(apk_file.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the smali code from apk file\n",
    "import subprocess\n",
    "def to_Smali(title):\n",
    "    if not os.path.exists('smali_files'):\n",
    "        os.mkdir('smali_files')\n",
    "    subprocess.run(['apktool', 'd', 'apk_files/'+ title + '.apk', '-o', 'smali_files/'+ title], capture_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Only Smali directory and xml files are left. Delete irrelevant directories\n",
    "import shutil\n",
    "def del_Dir(title):\n",
    "    all_directories = os.listdir('smali_files/'+title)\n",
    "    to_be_del = [directory for directory in all_directories if not (('.xml' in directory) or (\"smali\" in directory))]\n",
    "    for directory in to_be_del:\n",
    "        shutil.rmtree('smali_files/'+ title+ '/' + directory, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://apkpure.com/sitemap.xml'\n",
    "break_url = 'https://apkpure.com/sitemaps/group-49.xml.gz'\n",
    "num_to_sample = 1\n",
    "\n",
    "def Main(url, break_url, num_to_sample):\n",
    "    valid_urls = get_valid_urls(url, break_url)\n",
    "    sampled_urls = sample(num_to_sample, valid_urls)\n",
    "    for target_url in sampled_urls:\n",
    "        all_links = getAllLinksGZip(target_url)\n",
    "        download_link, title = getDownloadLink(all_links)\n",
    "        to_APK(download_link, title)\n",
    "        to_Smali(title)\n",
    "        del_Dir(title)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
